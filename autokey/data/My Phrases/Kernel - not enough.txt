Hi,

As my colleague previously mentioned, there is not enough data to explain the reboot. I've attached the sar statistics from Feb 08th (01036845-sar08.pdf). You can see the server operating normally, where no high IO nor memory pressure were happening. 

We advise you to configure and test kdump[1] along with some kernel tunables[2] in order to generate a vmcore[4] next time the system crashes. You can also crash the system manually with SysRq[3] to generate the vmcore.

This would help us to identify what caused the server to reboot if there was any serious issues with software or hardware. Unfortunately this won't help us in case the following was triggered

  * manual reboot
  * IPMI events
  * NMI events

The remote console (ILO, ILOM, DRAC etc) logs may also help. If you have further question do not hesitate to contact us.

  [1] - https://access.redhat.com/site/solutions/6038
  How to collect vmcore using kdump to troubleshoot kernel crashes, system hangs, or system reboots?

   https://access.redhat.com/labs/kdumphelper

  [2] - What panic parameters are available for use to panic a system when it is hanging or sluggish?
  https://access.redhat.com/site/solutions/459523

  [3] - https://access.redhat.com/articles/231663

  [4] - vmcore

A core dump is literally the contents of RAM - that is, all memory pages that are belonging to the kernel and userspace are dumped to a single file by an external kernel, known as the kdump kernel (from the project 'busybox' which is the kernel process).  Vmcore is not a log of sorts (although it contains a log) but rather the literal addresses of memory - from that we can align the kernelspace addresses to source and step through the last event of the crashed system.  In that regard, we can see:

-The last operations in dmesg
-Any values within any internal structures relative to the part of the kernel that crashed
-Full process stacks for any operation currently being run on-and-off the CPU's

From an analysis of the core and the last operations/values within the x86 CPU registers, we can determine why the system was panicked, in combination with source code.  This gives us absolute evidence concerning why a system has encountered an error, and exactly what it was doing at the time down to the lowest level.

Theoretically, that would mean all of your RAM would be dumped to a file, which would mean you have an extremely large file - by default, all userspace/process pages are dropped, since we likely can't compare them without the source of whatever it is that is failing and therefore is just extra space consumed.